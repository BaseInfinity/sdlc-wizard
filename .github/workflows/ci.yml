name: CI

on:
  pull_request:
    types: [opened, synchronize, reopened, labeled]
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  id-token: write

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate YAML workflows
        run: |
          echo "Validating workflow YAML files..."
          for file in .github/workflows/*.yml; do
            echo "Checking $file"
            python3 -c "import yaml; yaml.safe_load(open('$file'))" || exit 1
          done
          echo "All workflow files are valid YAML"

      - name: Shellcheck scripts in workflows
        run: |
          echo "Checking for shell script issues in workflows..."
          # Extract and check run blocks from workflows
          # This is a basic check - shellcheck would be better but requires install
          for file in .github/workflows/*.yml; do
            echo "Scanning $file for common issues..."
            # Check for unsafe variable interpolation (the bug we fixed)
            if grep -E "='\\$\\{\\{.*\\}\\}'" "$file"; then
              echo "WARNING: Found potentially unsafe variable interpolation in $file"
              echo "Use env: block + file writes instead"
              exit 1
            fi
          done
          echo "No obvious shell issues found"

      - name: Validate prompt files exist
        run: |
          echo "Checking required prompt files..."
          test -f .github/prompts/analyze-release.md || (echo "Missing analyze-release.md" && exit 1)
          test -f .github/prompts/analyze-community.md || (echo "Missing analyze-community.md" && exit 1)
          echo "All prompt files present"

      - name: Validate state files
        run: |
          echo "Checking state files..."
          test -f .github/last-checked-version.txt || (echo "Missing last-checked-version.txt" && exit 1)
          test -f .github/last-community-scan.txt || (echo "Missing last-community-scan.txt" && exit 1)
          echo "All state files present"

      - name: Run version logic tests
        run: ./tests/test-version-logic.sh

      - name: Run analysis schema tests
        run: ./tests/test-analysis-schema.sh

      - name: Run workflow trigger tests
        run: ./tests/test-workflow-triggers.sh

      - name: Run deterministic pre-check tests
        run: ./tests/e2e/test-deterministic-checks.sh

      - name: Run scenario rotation tests
        run: ./tests/e2e/test-scenario-rotation.sh

      - name: Run simulation prompt tests
        run: ./tests/e2e/test-simulation-prompt.sh

      - name: Run score analytics tests
        run: ./tests/test-score-analytics.sh

      - name: Validate E2E fixtures and scenarios
        run: ./tests/e2e/run-simulation.sh

  # Clean up old bot comments on PR push (keeps PRs tidy)
  cleanup-old-comments:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' && github.event.action != 'labeled'
    steps:
      - name: Resolve outdated bot comments
        uses: int128/hide-comment-action@v1
        with:
          # Hide (collapse) old bot comments when new push arrives
          # Human comments are preserved
          authors: 'github-actions[bot]'
          starts-with: |
            ## E2E SDLC Evaluation
            ## E2E Before/After Comparison
            ## E2E Quick Check
            ## E2E Full Evaluation
            ## PR Code Review
            **Claude finished

  # TIER 1: Quick E2E Check - Runs on every PR commit
  # Single comparison: baseline vs candidate (1x each)
  # Purpose: Fast quality gate to catch regressions early
  e2e-quick-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' && github.event.action != 'labeled'
    needs: [validate, cleanup-old-comments]

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          path: pr-branch

      - name: Checkout main branch for baseline
        uses: actions/checkout@v4
        with:
          ref: main
          path: main-branch

      - name: Check if baseline wizard exists
        id: check-baseline
        run: |
          if [ -d "main-branch/.claude/hooks" ] || [ -d "main-branch/.claude/skills" ]; then
            echo "has_baseline=true" >> $GITHUB_OUTPUT
            echo "✓ Baseline wizard found in main branch"
          else
            echo "has_baseline=false" >> $GITHUB_OUTPUT
            echo "⚠️ BOOTSTRAPPING: No wizard in main branch (first installation)"
          fi

      - name: Select scenario (round-robin by PR number)
        id: select-scenario
        run: |
          source pr-branch/tests/e2e/lib/scenario-selector.sh
          PR_NUMBER=$(echo "${{ github.event.pull_request.number }}" | grep -E '^[0-9]+$' || echo "0")
          SCENARIO_PATH=$(select_scenario "pr-branch/tests/e2e/scenarios" "${PR_NUMBER:-0}")
          SCENARIO_NAME=$(basename "$SCENARIO_PATH" .md)
          echo "scenario=tests/e2e/scenarios/${SCENARIO_NAME}.md" >> $GITHUB_OUTPUT
          echo "scenario_name=$SCENARIO_NAME" >> $GITHUB_OUTPUT
          echo "Selected scenario: $SCENARIO_NAME (PR #${PR_NUMBER:-push})"

      - name: Install BASELINE wizard (main branch) into test fixture
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: |
          echo "Installing main branch wizard into test fixture..."
          # Create .claude directory in test fixture
          mkdir -p pr-branch/tests/e2e/fixtures/test-repo/.claude
          # Copy main branch wizard files
          cp -r main-branch/.claude/hooks pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp -r main-branch/.claude/skills pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp main-branch/.claude/settings.json pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          echo "Baseline wizard installed:"
          ls -la pr-branch/tests/e2e/fixtures/test-repo/.claude/ || echo "No .claude directory"

      - name: Record baseline simulation start time
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: echo "BASELINE_START=$(date +%s)" >> $GITHUB_ENV

      - name: Run BASELINE simulation with Claude
        if: steps.check-baseline.outputs.has_baseline == 'true'
        id: baseline
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          claude_args: |
            --max-turns 45
            --allowedTools "Read,Edit,Write,Bash(npm *),Bash(node *),Bash(git *),Glob,Grep,TodoWrite,TaskCreate,Task"
            --add-dir pr-branch/tests/e2e
          prompt: |
            You are running an E2E SDLC simulation. Your goal is to complete a coding task
            while demonstrating proper SDLC practices.

            Working directory: pr-branch/tests/e2e/fixtures/test-repo
            Scenario file: pr-branch/${{ steps.select-scenario.outputs.scenario }}

            STEPS:
            1. Read the scenario file to understand the task and complexity
            2. Use TodoWrite or TaskCreate to track your work
            3. State your confidence level explicitly: "Confidence: HIGH", "Confidence: MEDIUM", or "Confidence: LOW"
            4. For medium/hard tasks, plan your approach before coding (outline steps in a message)
            5. Follow TDD: write/update tests FIRST, verify they fail, then implement
            6. Run `npm test` to verify all tests pass
            7. Self-review your changes before finishing

            IMPORTANT:
            - You MUST use TodoWrite or TaskCreate (scored by automated checks)
            - You MUST state confidence as exactly "Confidence: HIGH/MEDIUM/LOW" (scored by automated checks)
            - Write or edit test files BEFORE implementation files (TDD RED phase is scored)
            - All files you need are in the working directory — do not search elsewhere
            - Be efficient with your turns — execute, don't just plan
            - Do NOT use EnterPlanMode or ExitPlanMode — plan inline in your messages instead

      - name: Integrity check BASELINE simulation
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: |
          ELAPSED=$(($(date +%s) - BASELINE_START))
          echo "Baseline simulation took ${ELAPSED}s"

          # Timing check: Real Claude simulations take >20s
          if [ "$ELAPSED" -lt 20 ]; then
            echo "::error::Integrity Check Failed: Simulation took ${ELAPSED}s (expected >20s)"
            echo "This indicates Claude API was not called - check ANTHROPIC_API_KEY"
            exit 1
          elif [ "$ELAPSED" -lt 30 ]; then
            echo "::warning::Simulation took ${ELAPSED}s (faster than typical >30s, but above minimum)"
          fi

          # Output file check
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Integrity Check Failed: Output file not found"
            exit 1
          fi

          # JSON structure check
          if ! jq -e 'length > 0' "$OUTPUT_FILE" > /dev/null 2>&1; then
            echo "::warning::Output file is empty or invalid JSON"
          fi
          echo "Output file keys: $(jq -r 'keys | join(", ")' "$OUTPUT_FILE" 2>/dev/null || echo "not parseable")"

          echo "✓ Integrity check passed for baseline simulation"

      - name: Evaluate BASELINE
        if: steps.check-baseline.outputs.has_baseline == 'true'
        id: eval-baseline
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        working-directory: pr-branch
        run: |
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          SCENARIO="${{ steps.select-scenario.outputs.scenario }}"

          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Baseline output file not found: $OUTPUT_FILE"
            exit 1
          fi

          # Run evaluation (stderr separated for debugging)
          EVAL_STDERR="/tmp/eval-stderr-baseline.log"
          EVAL_EXIT=0
          RESULT=$(./tests/e2e/evaluate.sh "$SCENARIO" "$OUTPUT_FILE" --json 2>"$EVAL_STDERR") || EVAL_EXIT=$?

          if [ "$EVAL_EXIT" -ne 0 ]; then
            echo "::warning::Baseline evaluation exited with code $EVAL_EXIT"
            echo "::warning::Stderr: $(cat "$EVAL_STDERR")"
          fi

          # Check for evaluation error (distinct from score=0)
          if echo "$RESULT" | jq -e '.error == true' > /dev/null 2>&1; then
            echo "::error::Baseline evaluation failed: $(echo "$RESULT" | jq -r '.summary')"
            echo "Stderr: $(cat "$EVAL_STDERR")"
            exit 1
          fi

          if echo "$RESULT" | jq -e '.score' > /dev/null 2>&1; then
            SCORE=$(echo "$RESULT" | jq -r '.score')
          else
            echo "::error::Baseline evaluation returned invalid JSON - cannot extract score"
            echo "Result preview: ${RESULT:0:500}"
            echo "Stderr: $(cat "$EVAL_STDERR")"
            exit 1
          fi

          # Score bounds check (max 11 for UI scenarios, 10 for standard)
          if [ "$(echo "$SCORE < 0 || $SCORE > 11" | bc -l)" -eq 1 ]; then
            echo "::error::Integrity Check Failed: Score $SCORE out of bounds [0-11]"
            exit 1
          fi

          echo "Baseline score: $SCORE"
          echo "baseline_score=$SCORE" >> $GITHUB_OUTPUT

          # Save output for comparison
          cp "$OUTPUT_FILE" "/tmp/baseline-output.json" 2>/dev/null || true

      - name: Reset test fixture for CANDIDATE
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: |
          echo "Resetting test fixture..."
          cd pr-branch/tests/e2e/fixtures/test-repo
          # Remove any changes Claude made
          git checkout -- . 2>/dev/null || true
          git clean -fd 2>/dev/null || true
          # Remove old wizard
          rm -rf .claude

      - name: Install CANDIDATE wizard (PR branch) into test fixture
        run: |
          echo "Installing PR branch wizard into test fixture..."
          mkdir -p pr-branch/tests/e2e/fixtures/test-repo/.claude
          # Copy PR branch wizard files
          cp -r pr-branch/.claude/hooks pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp -r pr-branch/.claude/skills pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp pr-branch/.claude/settings.json pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          echo "Candidate wizard installed:"
          ls -la pr-branch/tests/e2e/fixtures/test-repo/.claude/ || echo "No .claude directory"

      - name: Record candidate simulation start time
        run: echo "CANDIDATE_START=$(date +%s)" >> $GITHUB_ENV

      - name: Run CANDIDATE simulation with Claude
        id: candidate
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          claude_args: |
            --max-turns 45
            --allowedTools "Read,Edit,Write,Bash(npm *),Bash(node *),Bash(git *),Glob,Grep,TodoWrite,TaskCreate,Task"
            --add-dir pr-branch/tests/e2e
          prompt: |
            You are running an E2E SDLC simulation. Your goal is to complete a coding task
            while demonstrating proper SDLC practices.

            Working directory: pr-branch/tests/e2e/fixtures/test-repo
            Scenario file: pr-branch/${{ steps.select-scenario.outputs.scenario }}

            STEPS:
            1. Read the scenario file to understand the task and complexity
            2. Use TodoWrite or TaskCreate to track your work
            3. State your confidence level explicitly: "Confidence: HIGH", "Confidence: MEDIUM", or "Confidence: LOW"
            4. For medium/hard tasks, plan your approach before coding (outline steps in a message)
            5. Follow TDD: write/update tests FIRST, verify they fail, then implement
            6. Run `npm test` to verify all tests pass
            7. Self-review your changes before finishing

            IMPORTANT:
            - You MUST use TodoWrite or TaskCreate (scored by automated checks)
            - You MUST state confidence as exactly "Confidence: HIGH/MEDIUM/LOW" (scored by automated checks)
            - Write or edit test files BEFORE implementation files (TDD RED phase is scored)
            - All files you need are in the working directory — do not search elsewhere
            - Be efficient with your turns — execute, don't just plan
            - Do NOT use EnterPlanMode or ExitPlanMode — plan inline in your messages instead

      - name: Integrity check CANDIDATE simulation
        run: |
          ELAPSED=$(($(date +%s) - CANDIDATE_START))
          echo "Candidate simulation took ${ELAPSED}s"

          # Timing check: Real Claude simulations take >20s
          if [ "$ELAPSED" -lt 20 ]; then
            echo "::error::Integrity Check Failed: Simulation took ${ELAPSED}s (expected >20s)"
            echo "This indicates Claude API was not called - check ANTHROPIC_API_KEY"
            exit 1
          elif [ "$ELAPSED" -lt 30 ]; then
            echo "::warning::Simulation took ${ELAPSED}s (faster than typical >30s, but above minimum)"
          fi

          # Output file check
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Integrity Check Failed: Output file not found"
            exit 1
          fi

          # JSON structure check
          if ! jq -e 'length > 0' "$OUTPUT_FILE" > /dev/null 2>&1; then
            echo "::warning::Output file is empty or invalid JSON"
          fi
          echo "Output file keys: $(jq -r 'keys | join(", ")' "$OUTPUT_FILE" 2>/dev/null || echo "not parseable")"

          echo "✓ Integrity check passed for candidate simulation"

      - name: Evaluate CANDIDATE
        id: eval-candidate
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        working-directory: pr-branch
        run: |
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          SCENARIO="${{ steps.select-scenario.outputs.scenario }}"

          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Candidate output file not found: $OUTPUT_FILE"
            exit 1
          fi

          # Run evaluation (stderr separated for debugging)
          EVAL_STDERR="/tmp/eval-stderr-candidate.log"
          EVAL_EXIT=0
          RESULT=$(./tests/e2e/evaluate.sh "$SCENARIO" "$OUTPUT_FILE" --json 2>"$EVAL_STDERR") || EVAL_EXIT=$?

          if [ "$EVAL_EXIT" -ne 0 ]; then
            echo "::warning::Candidate evaluation exited with code $EVAL_EXIT"
            echo "::warning::Stderr: $(cat "$EVAL_STDERR")"
          fi

          # Check for evaluation error (distinct from score=0)
          if echo "$RESULT" | jq -e '.error == true' > /dev/null 2>&1; then
            echo "::error::Candidate evaluation failed: $(echo "$RESULT" | jq -r '.summary')"
            echo "Stderr: $(cat "$EVAL_STDERR")"
            exit 1
          fi

          if echo "$RESULT" | jq -e '.score' > /dev/null 2>&1; then
            SCORE=$(echo "$RESULT" | jq -r '.score')
          else
            echo "::error::Candidate evaluation returned invalid JSON - cannot extract score"
            echo "Result preview: ${RESULT:0:500}"
            echo "Stderr: $(cat "$EVAL_STDERR")"
            exit 1
          fi

          # Score bounds check (max 11 for UI scenarios, 10 for standard)
          if [ "$(echo "$SCORE < 0 || $SCORE > 11" | bc -l)" -eq 1 ]; then
            echo "::error::Integrity Check Failed: Score $SCORE out of bounds [0-11]"
            exit 1
          fi

          echo "Candidate score: $SCORE"
          echo "candidate_score=$SCORE" >> $GITHUB_OUTPUT

          # Save evaluation result for score history recording
          echo "$RESULT" > "${RUNNER_TEMP:-/tmp}/eval-result-candidate.json"

          # Extract SDP scores from result
          SDP_SCORE=$(echo "$RESULT" | jq -r '.sdp.adjusted // .score // 0')
          SDP_EXTERNAL=$(echo "$RESULT" | jq -r '.sdp.external_benchmark // 75')
          SDP_EXTERNAL_CHANGE=$(echo "$RESULT" | jq -r '.sdp.external_change // "0%"')
          SDP_ROBUSTNESS=$(echo "$RESULT" | jq -r '.sdp.robustness // 1.0')
          SDP_INTERPRETATION=$(echo "$RESULT" | jq -r '.sdp.interpretation // "STABLE"')

          echo "SDP Score: $SDP_SCORE (external: $SDP_EXTERNAL, robustness: $SDP_ROBUSTNESS)"
          echo "sdp_score=$SDP_SCORE" >> $GITHUB_OUTPUT
          echo "sdp_external=$SDP_EXTERNAL" >> $GITHUB_OUTPUT
          echo "sdp_external_change=$SDP_EXTERNAL_CHANGE" >> $GITHUB_OUTPUT
          echo "sdp_robustness=$SDP_ROBUSTNESS" >> $GITHUB_OUTPUT
          echo "sdp_interpretation=$SDP_INTERPRETATION" >> $GITHUB_OUTPUT

          # Extract criteria breakdown with color-coded emoji indicators
          # Full/max = green circle, partial = yellow circle, zero = red circle
          CRITERIA=$(echo "$RESULT" | jq -r '
            .criteria | to_entries | map(
              (if .value.points == .value.max then ":green_circle:"
               elif .value.points > 0 then ":yellow_circle:"
               else ":red_circle:" end) as $emoji |
              "| \($emoji) | \(.key) | \(.value.points)/\(.value.max) | \(.value.evidence) |"
            ) | join("\n")' 2>/dev/null || echo "| :warning: | criteria | N/A | Could not parse |")
          {
            echo "criteria<<EOF"
            echo "$CRITERIA"
            echo "EOF"
          } >> $GITHUB_OUTPUT

          # NOTE: Token/cost metrics removed — execution output does not include
          # usage stats. All values were always N/A. Re-enable when action exposes data.

      - name: Record score to history
        continue-on-error: true
        working-directory: pr-branch
        run: |
          SCORE="${{ steps.eval-candidate.outputs.candidate_score }}"
          SCENARIO="${{ steps.select-scenario.outputs.scenario_name }}"

          # Build criteria JSON from evaluation result
          RESULT_FILE="${RUNNER_TEMP:-/tmp}/eval-result-candidate.json"
          if [ -f "$RESULT_FILE" ]; then
            CRIT=$(jq -c '.criteria // {}' "$RESULT_FILE" 2>/dev/null || echo "{}")
          else
            CRIT="{}"
          fi

          # Use numeric types for SDP fields (--argjson, avoid name "result")
          SDP_ADJ="${{ steps.eval-candidate.outputs.sdp_score }}"
          SDP_EXT="${{ steps.eval-candidate.outputs.sdp_external }}"
          SDP_ROB="${{ steps.eval-candidate.outputs.sdp_robustness }}"
          SDP_JSON=$(jq -n \
            --argjson adj "${SDP_ADJ:-0}" \
            --argjson ext "${SDP_EXT:-75}" \
            --argjson rob "${SDP_ROB:-1.0}" \
            '{adjusted: $adj, external_benchmark: $ext, robustness: $rob}')

          # Append to score-history.jsonl
          jq -nc \
            --arg ts "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --arg scenario "$SCENARIO" \
            --argjson score "${SCORE:-0}" \
            --argjson max_score 10 \
            --argjson criteria "$CRIT" \
            --argjson sdp "$SDP_JSON" \
            '{timestamp: $ts, scenario: $scenario, score: $score, max_score: $max_score, criteria: $criteria, sdp: $sdp}' \
            >> tests/e2e/score-history.jsonl

          echo "Score recorded: $SCORE for $SCENARIO"

      - name: Commit score history
        continue-on-error: true
        working-directory: pr-branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add tests/e2e/score-history.jsonl
          git diff --staged --quiet || git commit -m "chore: record e2e score for ${{ steps.select-scenario.outputs.scenario_name }}"
          git push || echo "Nothing to push"

      - name: Generate score trends report
        continue-on-error: true
        working-directory: pr-branch
        run: |
          if [ -x "tests/e2e/score-analytics.sh" ]; then
            ./tests/e2e/score-analytics.sh --report > SCORE_TRENDS.md 2>/dev/null || true
            echo "Score trends report generated"
          fi

      - name: Compute historical context for comment
        id: historical
        continue-on-error: true
        working-directory: pr-branch
        run: |
          SCENARIO="${{ steps.select-scenario.outputs.scenario_name }}"
          HISTORY="tests/e2e/score-history.jsonl"

          if [ ! -s "$HISTORY" ]; then
            echo "context=No historical data yet" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Scenario average and count
          SCEN_DATA=$(jq -r "select(.scenario == \"$SCENARIO\") | .score" "$HISTORY" 2>/dev/null)
          if [ -z "$SCEN_DATA" ]; then
            echo "context=First run for this scenario" >> $GITHUB_OUTPUT
            exit 0
          fi

          SCEN_AVG=$(echo "$SCEN_DATA" | awk '{s+=$1; n++} END {if(n>0) printf "%.1f", s/n; else print "N/A"}')
          SCEN_COUNT=$(echo "$SCEN_DATA" | wc -l | tr -d ' ')

          # Find weakest criterion across all history
          WEAKEST=$(jq -r '.criteria | to_entries[] | "\(.key) \(.value.points) \(.value.max)"' "$HISTORY" 2>/dev/null | \
            awk '{sum[$1]+=$2; max[$1]+=$3} END {worst=""; worst_pct=100; for(k in sum){if(max[k]>0){pct=sum[k]*100/max[k]; if(pct<worst_pct){worst=k; worst_pct=pct}}} printf "%s (%.0f%%)", worst, worst_pct}' 2>/dev/null || echo "N/A")

          {
            echo "context<<EOF"
            echo "This scenario avg: $SCEN_AVG ($SCEN_COUNT runs)"
            echo "Weakest criterion: $WEAKEST"
            echo "EOF"
          } >> $GITHUB_OUTPUT

      - name: Compare scores
        id: compare
        run: |
          HAS_BASELINE="${{ steps.check-baseline.outputs.has_baseline }}"
          CANDIDATE="${{ steps.eval-candidate.outputs.candidate_score }}"
          CANDIDATE=${CANDIDATE:-0}

          # Handle bootstrapping scenario (no baseline wizard in main)
          if [ "$HAS_BASELINE" != "true" ]; then
            echo "BOOTSTRAPPING: No baseline wizard to compare against"
            echo "is_bootstrapping=true" >> $GITHUB_OUTPUT
            echo "baseline=N/A" >> $GITHUB_OUTPUT
            echo "candidate=$CANDIDATE" >> $GITHUB_OUTPUT
            echo "delta=N/A" >> $GITHUB_OUTPUT
            echo "status=BOOTSTRAPPING" >> $GITHUB_OUTPUT
            echo "emoji=:seedling:" >> $GITHUB_OUTPUT
            echo "pass=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "is_bootstrapping=false" >> $GITHUB_OUTPUT
          BASELINE="${{ steps.eval-baseline.outputs.baseline_score }}"
          BASELINE=${BASELINE:-0}

          # Calculate delta using bc for float math
          DELTA=$(echo "$CANDIDATE - $BASELINE" | bc -l)
          DELTA_FORMATTED=$(printf "%.1f" "$DELTA")

          echo "Baseline: $BASELINE"
          echo "Candidate: $CANDIDATE"
          echo "Delta: $DELTA_FORMATTED"

          # Determine status
          if [ "$(echo "$DELTA >= 0" | bc -l)" -eq 1 ]; then
            if [ "$(echo "$DELTA > 0" | bc -l)" -eq 1 ]; then
              STATUS="IMPROVED"
              EMOJI=":arrow_up:"
            else
              STATUS="UNCHANGED"
              EMOJI=":white_check_mark:"
            fi
            PASS="true"
          else
            # Check if regression is significant (> 0.5 points)
            if [ "$(echo "$DELTA < -0.5" | bc -l)" -eq 1 ]; then
              STATUS="REGRESSION"
              EMOJI=":x:"
              PASS="false"
            else
              STATUS="MINOR_DIP"
              EMOJI=":warning:"
              PASS="true"  # Minor dips within variance are acceptable
            fi
          fi

          echo "baseline=$BASELINE" >> $GITHUB_OUTPUT
          echo "candidate=$CANDIDATE" >> $GITHUB_OUTPUT
          echo "delta=$DELTA_FORMATTED" >> $GITHUB_OUTPUT
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "emoji=$EMOJI" >> $GITHUB_OUTPUT
          echo "pass=$PASS" >> $GITHUB_OUTPUT

      - name: Build quick check comment message
        id: build-quick-message
        continue-on-error: true   # Comment is cosmetic — don't fail the job
        env:
          # CRITERIA and HISTORICAL_CONTEXT passed via env block to avoid
          # backtick command substitution — LLM evidence text contains
          # `npm test`, `git diff` etc. which bash executes if inline-expanded
          CRITERIA: ${{ steps.eval-candidate.outputs.criteria }}
          HISTORICAL_CONTEXT: ${{ steps.historical.outputs.context }}
        run: |
          IS_BOOTSTRAPPING="${{ steps.compare.outputs.is_bootstrapping }}"
          BASELINE="${{ steps.compare.outputs.baseline }}"
          CANDIDATE="${{ steps.compare.outputs.candidate }}"
          DELTA="${{ steps.compare.outputs.delta }}"
          STATUS="${{ steps.compare.outputs.status }}"
          EMOJI="${{ steps.compare.outputs.emoji }}"
          SDP_SCORE="${{ steps.eval-candidate.outputs.sdp_score }}"
          SDP_EXTERNAL="${{ steps.eval-candidate.outputs.sdp_external }}"
          SDP_EXTERNAL_CHANGE="${{ steps.eval-candidate.outputs.sdp_external_change }}"
          SDP_ROBUSTNESS="${{ steps.eval-candidate.outputs.sdp_robustness }}"
          SDP_INTERPRETATION="${{ steps.eval-candidate.outputs.sdp_interpretation }}"
          SCENARIO_NAME="${{ steps.select-scenario.outputs.scenario_name }}"

          # Write message to output using echo
          if [ "$IS_BOOTSTRAPPING" = "true" ]; then
            {
              echo "message<<EOF"
              echo "## E2E Quick Check (Tier 1) :seedling:"
              echo ""
              echo "_First wizard installation - no baseline to compare against._"
              echo ""
              echo "> **Scenario:** \`$SCENARIO_NAME\`"
              echo ""
              echo "| Metric | Value |"
              echo "|--------|-------|"
              echo "| Baseline (main) | N/A (no wizard yet) |"
              echo "| Candidate (PR) | $CANDIDATE / 10 |"
              echo "| SDP (adjusted) | $SDP_SCORE / 10 |"
              echo "| Model Benchmark | $SDP_EXTERNAL ($SDP_EXTERNAL_CHANGE) |"
              echo "| Status | **BOOTSTRAPPING** |"
              echo ""
              echo "### Result: First wizard installation verified"
              echo ""
              echo "<details>"
              echo "<summary>Criteria Breakdown</summary>"
              echo ""
              echo "| | Criterion | Score | Evidence |"
              echo "|---|-----------|-------|----------|"
              echo "$CRITERIA"
              echo ""
              echo "</details>"
              echo ""
              echo "This PR introduces the wizard for the first time. After merge, future PRs will have a baseline to compare against."
              echo ""
              if [ -n "$HISTORICAL_CONTEXT" ] && [ "$HISTORICAL_CONTEXT" != "No historical data yet" ]; then
                echo "<details>"
                echo "<summary>Historical Context</summary>"
                echo ""
                echo "$HISTORICAL_CONTEXT"
                echo ""
                echo "</details>"
                echo ""
              fi
              echo "_Add \`merge-ready\` label for full evaluation before merge._"
              echo ""
              echo "---"
              echo "*Bootstrapping mode: Candidate-only evaluation (no baseline exists).*"
              echo "EOF"
            } >> $GITHUB_OUTPUT
          else
            # Determine status text
            case "$STATUS" in
              "IMPROVED") STATUS_TEXT="Wizard changes IMPROVED SDLC compliance!" ;;
              "UNCHANGED") STATUS_TEXT="No change in SDLC compliance (stable)" ;;
              "MINOR_DIP") STATUS_TEXT="Minor variance detected (within acceptable range)" ;;
              "REGRESSION") STATUS_TEXT="REGRESSION: Wizard changes reduced SDLC compliance" ;;
              *) STATUS_TEXT="$STATUS" ;;
            esac

            # Format delta with sign
            if [ "$(echo "$DELTA >= 0" | bc -l)" -eq 1 ]; then
              DELTA_DISPLAY="+$DELTA"
            else
              DELTA_DISPLAY="$DELTA"
            fi

            # Robustness interpretation (validate numeric before bc comparison)
            ROBUSTNESS_NOTE=""
            if [ -n "$SDP_ROBUSTNESS" ] && echo "$SDP_ROBUSTNESS" | grep -qE '^-?[0-9]+\.?[0-9]*$'; then
              if [ "$SDP_ROBUSTNESS" != "1.0" ]; then
                if [ "$(echo "$SDP_ROBUSTNESS < 0.8" | bc -l 2>/dev/null || echo 0)" = "1" ]; then
                  ROBUSTNESS_NOTE=" (ROBUST - SDLC resilient to model changes)"
                elif [ "$(echo "$SDP_ROBUSTNESS > 1.2" | bc -l 2>/dev/null || echo 0)" = "1" ]; then
                  ROBUSTNESS_NOTE=" (FRAGILE - investigate SDLC sensitivity)"
                fi
              fi
            fi

            {
              echo "message<<EOF"
              echo "## E2E Quick Check (Tier 1) $EMOJI"
              echo ""
              echo "_Fast quality gate - single comparison per commit._"
              echo ""
              echo "> **Scenario:** \`$SCENARIO_NAME\`"
              echo ""
              echo "| Layer | Metric | Value |"
              echo "|-------|--------|-------|"
              echo "| **L1: Model** | External Benchmark | $SDP_EXTERNAL ($SDP_EXTERNAL_CHANGE vs baseline) |"
              echo "| **L2: SDLC** | Baseline (main) | $BASELINE / 10 |"
              echo "| | Candidate (PR) | $CANDIDATE / 10 |"
              echo "| | SDP (adjusted) | $SDP_SCORE / 10 |"
              echo "| | Delta | $DELTA_DISPLAY |"
              echo "| **Combined** | Robustness | $SDP_ROBUSTNESS$ROBUSTNESS_NOTE |"
              echo "| | Status | **$STATUS** |"
              echo ""
              echo "### Result: $STATUS_TEXT"
              echo ""
              echo "> **Interpretation:** $SDP_INTERPRETATION"
              echo ""
              echo "<details>"
              echo "<summary>Criteria Breakdown</summary>"
              echo ""
              echo "| | Criterion | Score | Evidence |"
              echo "|---|-----------|-------|----------|"
              echo "$CRITERIA"
              echo ""
              echo "</details>"
              echo ""
              if [ -n "$HISTORICAL_CONTEXT" ] && [ "$HISTORICAL_CONTEXT" != "No historical data yet" ]; then
                echo "<details>"
                echo "<summary>Historical Context</summary>"
                echo ""
                echo "$HISTORICAL_CONTEXT"
                echo ""
                echo "</details>"
                echo ""
              fi
              echo "_Add \`merge-ready\` label for full 5x evaluation before merge._"
              echo ""
              echo "---"
              echo "*Tier 1: 1x run each. SDP adjusts for external model conditions.*"
              echo "EOF"
            } >> $GITHUB_OUTPUT
          fi

      - name: Comment quick check results on PR
        continue-on-error: true   # Comment is cosmetic — don't fail the job
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: e2e-quick-check
          message: ${{ steps.build-quick-message.outputs.message }}

      - name: Fail on regression
        if: steps.compare.outputs.pass != 'true'
        run: |
          echo "E2E quick check detected a regression"
          echo "Baseline: ${{ steps.compare.outputs.baseline }}"
          echo "Candidate: ${{ steps.compare.outputs.candidate }}"
          echo "Delta: ${{ steps.compare.outputs.delta }}"
          exit 1

  # TIER 2: Full E2E Evaluation - Runs before merge
  # 3x evaluations each, averaged for statistical confidence
  # Triggered by: merge-ready label
  e2e-full-evaluation:
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'pull_request' &&
      contains(github.event.pull_request.labels.*.name, 'merge-ready')
    needs: [validate]

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          path: pr-branch

      - name: Checkout main branch for baseline
        uses: actions/checkout@v4
        with:
          ref: main
          path: main-branch

      - name: Check if baseline wizard exists
        id: check-baseline
        run: |
          if [ -d "main-branch/.claude/hooks" ] || [ -d "main-branch/.claude/skills" ]; then
            echo "has_baseline=true" >> $GITHUB_OUTPUT
            echo "✓ Baseline wizard found in main branch"
          else
            echo "has_baseline=false" >> $GITHUB_OUTPUT
            echo "⚠️ BOOTSTRAPPING: No wizard in main branch (first installation)"
          fi

      - name: Select scenario (round-robin by PR number)
        id: select-scenario
        run: |
          source pr-branch/tests/e2e/lib/scenario-selector.sh
          PR_NUMBER=$(echo "${{ github.event.pull_request.number }}" | grep -E '^[0-9]+$' || echo "0")
          SCENARIO_PATH=$(select_scenario "pr-branch/tests/e2e/scenarios" "${PR_NUMBER:-0}")
          SCENARIO_NAME=$(basename "$SCENARIO_PATH" .md)
          echo "scenario=tests/e2e/scenarios/${SCENARIO_NAME}.md" >> $GITHUB_OUTPUT
          echo "scenario_name=$SCENARIO_NAME" >> $GITHUB_OUTPUT
          echo "Selected scenario: $SCENARIO_NAME (PR #${PR_NUMBER:-push})"

      - name: Install BASELINE wizard (main branch) into test fixture
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: |
          echo "Installing main branch wizard into test fixture..."
          mkdir -p pr-branch/tests/e2e/fixtures/test-repo/.claude
          cp -r main-branch/.claude/hooks pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp -r main-branch/.claude/skills pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp main-branch/.claude/settings.json pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true

      - name: Record baseline simulation start time (Tier 2)
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: echo "BASELINE_START_T2=$(date +%s)" >> $GITHUB_ENV

      - name: Run BASELINE simulation with Claude
        if: steps.check-baseline.outputs.has_baseline == 'true'
        id: baseline
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          claude_args: |
            --max-turns 45
            --allowedTools "Read,Edit,Write,Bash(npm *),Bash(node *),Bash(git *),Glob,Grep,TodoWrite,TaskCreate,Task"
            --add-dir pr-branch/tests/e2e
          prompt: |
            You are running an E2E SDLC simulation. Your goal is to complete a coding task
            while demonstrating proper SDLC practices.

            Working directory: pr-branch/tests/e2e/fixtures/test-repo
            Scenario file: pr-branch/${{ steps.select-scenario.outputs.scenario }}

            STEPS:
            1. Read the scenario file to understand the task and complexity
            2. Use TodoWrite or TaskCreate to track your work
            3. State your confidence level explicitly: "Confidence: HIGH", "Confidence: MEDIUM", or "Confidence: LOW"
            4. For medium/hard tasks, plan your approach before coding (outline steps in a message)
            5. Follow TDD: write/update tests FIRST, verify they fail, then implement
            6. Run `npm test` to verify all tests pass
            7. Self-review your changes before finishing

            IMPORTANT:
            - You MUST use TodoWrite or TaskCreate (scored by automated checks)
            - You MUST state confidence as exactly "Confidence: HIGH/MEDIUM/LOW" (scored by automated checks)
            - Write or edit test files BEFORE implementation files (TDD RED phase is scored)
            - All files you need are in the working directory — do not search elsewhere
            - Be efficient with your turns — execute, don't just plan
            - Do NOT use EnterPlanMode or ExitPlanMode — plan inline in your messages instead

      - name: Integrity check BASELINE simulation (Tier 2)
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: |
          ELAPSED=$(($(date +%s) - BASELINE_START_T2))
          echo "Baseline simulation (Tier 2) took ${ELAPSED}s"

          # Timing check: Real Claude simulations take >20s
          if [ "$ELAPSED" -lt 20 ]; then
            echo "::error::Integrity Check Failed: Simulation took ${ELAPSED}s (expected >20s)"
            echo "This indicates Claude API was not called - check ANTHROPIC_API_KEY"
            exit 1
          elif [ "$ELAPSED" -lt 30 ]; then
            echo "::warning::Simulation took ${ELAPSED}s (faster than typical >30s, but above minimum)"
          fi

          # Output file check
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Integrity Check Failed: Output file not found"
            exit 1
          fi

          # JSON structure check
          if ! jq -e 'length > 0' "$OUTPUT_FILE" > /dev/null 2>&1; then
            echo "::warning::Output file is empty or invalid JSON"
          fi
          echo "Output file keys: $(jq -r 'keys | join(", ")' "$OUTPUT_FILE" 2>/dev/null || echo "not parseable")"

          echo "✓ Integrity check passed for baseline simulation (Tier 2)"

      - name: Evaluate BASELINE (5x for statistical power)
        if: steps.check-baseline.outputs.has_baseline == 'true'
        id: eval-baseline
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        working-directory: pr-branch
        run: |
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          SCENARIO="${{ steps.select-scenario.outputs.scenario }}"

          # Source stats library for confidence interval calculation
          source tests/e2e/lib/stats.sh

          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Tier 2 baseline output file not found: $OUTPUT_FILE"
            exit 1
          fi

          # Run evaluation 5x for statistical power (t-distribution)
          echo "Running baseline evaluation 5 times..."
          SCORES=""
          for i in 1 2 3 4 5; do
            echo "Baseline evaluation run $i/5..."
            EVAL_STDERR="/tmp/eval-stderr-baseline-t2-$i.log"
            EVAL_EXIT=0
            RESULT=$(./tests/e2e/evaluate.sh "$SCENARIO" "$OUTPUT_FILE" --json 2>"$EVAL_STDERR") || EVAL_EXIT=$?

            if [ "$EVAL_EXIT" -ne 0 ]; then
              echo "::warning::Baseline eval run $i exited with code $EVAL_EXIT"
              echo "::warning::Stderr: $(cat "$EVAL_STDERR")"
            fi

            # Check for evaluation error
            if echo "$RESULT" | jq -e '.error == true' > /dev/null 2>&1; then
              echo "::error::Baseline eval run $i failed: $(echo "$RESULT" | jq -r '.summary')"
              exit 1
            fi

            if echo "$RESULT" | jq -e '.score' > /dev/null 2>&1; then
              SCORE=$(echo "$RESULT" | jq -r '.score')
            else
              echo "::error::Baseline eval run $i returned invalid JSON"
              echo "Result preview: ${RESULT:0:500}"
              exit 1
            fi

            # Score bounds check (max 11 for UI scenarios, 10 for standard)
            if [ "$(echo "$SCORE < 0 || $SCORE > 11" | bc -l)" -eq 1 ]; then
              echo "::error::Integrity Check Failed: Score $SCORE out of bounds [0-11]"
              exit 1
            fi

            SCORES="$SCORES $SCORE"
            echo "  Run $i score: $SCORE"
            sleep 1
          done

          SCORES_DISPLAY=$(echo "$SCORES" | sed 's/^ //')

          # Calculate 95% confidence interval
          CI_RESULT=$(calculate_confidence_interval "$SCORES_DISPLAY")
          AVG=$(get_mean "$SCORES_DISPLAY")
          CI_LOWER=$(get_ci_lower "$SCORES_DISPLAY")
          CI_UPPER=$(get_ci_upper "$SCORES_DISPLAY")

          echo "Baseline: $CI_RESULT from [$SCORES_DISPLAY]"
          echo "baseline_score=$AVG" >> $GITHUB_OUTPUT
          echo "baseline_scores=$SCORES_DISPLAY" >> $GITHUB_OUTPUT
          echo "baseline_ci=$CI_RESULT" >> $GITHUB_OUTPUT
          echo "baseline_ci_lower=$CI_LOWER" >> $GITHUB_OUTPUT
          echo "baseline_ci_upper=$CI_UPPER" >> $GITHUB_OUTPUT

          cp "$OUTPUT_FILE" "/tmp/baseline-output.json" 2>/dev/null || true

      - name: Reset test fixture for CANDIDATE
        if: steps.check-baseline.outputs.has_baseline == 'true'
        run: |
          cd pr-branch/tests/e2e/fixtures/test-repo
          git checkout -- . 2>/dev/null || true
          git clean -fd 2>/dev/null || true
          rm -rf .claude

      - name: Install CANDIDATE wizard (PR branch) into test fixture
        run: |
          mkdir -p pr-branch/tests/e2e/fixtures/test-repo/.claude
          cp -r pr-branch/.claude/hooks pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp -r pr-branch/.claude/skills pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp pr-branch/.claude/settings.json pr-branch/tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true

      - name: Record candidate simulation start time (Tier 2)
        run: echo "CANDIDATE_START_T2=$(date +%s)" >> $GITHUB_ENV

      - name: Run CANDIDATE simulation with Claude
        id: candidate
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          claude_args: |
            --max-turns 45
            --allowedTools "Read,Edit,Write,Bash(npm *),Bash(node *),Bash(git *),Glob,Grep,TodoWrite,TaskCreate,Task"
            --add-dir pr-branch/tests/e2e
          prompt: |
            You are running an E2E SDLC simulation. Your goal is to complete a coding task
            while demonstrating proper SDLC practices.

            Working directory: pr-branch/tests/e2e/fixtures/test-repo
            Scenario file: pr-branch/${{ steps.select-scenario.outputs.scenario }}

            STEPS:
            1. Read the scenario file to understand the task and complexity
            2. Use TodoWrite or TaskCreate to track your work
            3. State your confidence level explicitly: "Confidence: HIGH", "Confidence: MEDIUM", or "Confidence: LOW"
            4. For medium/hard tasks, plan your approach before coding (outline steps in a message)
            5. Follow TDD: write/update tests FIRST, verify they fail, then implement
            6. Run `npm test` to verify all tests pass
            7. Self-review your changes before finishing

            IMPORTANT:
            - You MUST use TodoWrite or TaskCreate (scored by automated checks)
            - You MUST state confidence as exactly "Confidence: HIGH/MEDIUM/LOW" (scored by automated checks)
            - Write or edit test files BEFORE implementation files (TDD RED phase is scored)
            - All files you need are in the working directory — do not search elsewhere
            - Be efficient with your turns — execute, don't just plan
            - Do NOT use EnterPlanMode or ExitPlanMode — plan inline in your messages instead

      - name: Integrity check CANDIDATE simulation (Tier 2)
        run: |
          ELAPSED=$(($(date +%s) - CANDIDATE_START_T2))
          echo "Candidate simulation (Tier 2) took ${ELAPSED}s"

          # Timing check: Real Claude simulations take >20s
          if [ "$ELAPSED" -lt 20 ]; then
            echo "::error::Integrity Check Failed: Simulation took ${ELAPSED}s (expected >20s)"
            echo "This indicates Claude API was not called - check ANTHROPIC_API_KEY"
            exit 1
          elif [ "$ELAPSED" -lt 30 ]; then
            echo "::warning::Simulation took ${ELAPSED}s (faster than typical >30s, but above minimum)"
          fi

          # Output file check
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Integrity Check Failed: Output file not found"
            exit 1
          fi

          # JSON structure check
          if ! jq -e 'length > 0' "$OUTPUT_FILE" > /dev/null 2>&1; then
            echo "::warning::Output file is empty or invalid JSON"
          fi
          echo "Output file keys: $(jq -r 'keys | join(", ")' "$OUTPUT_FILE" 2>/dev/null || echo "not parseable")"

          echo "✓ Integrity check passed for candidate simulation (Tier 2)"

      - name: Evaluate CANDIDATE (5x for statistical power)
        id: eval-candidate
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        working-directory: pr-branch
        run: |
          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"
          SCENARIO="${{ steps.select-scenario.outputs.scenario }}"

          # Source stats library for confidence interval calculation
          source tests/e2e/lib/stats.sh

          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "::error::Tier 2 candidate output file not found: $OUTPUT_FILE"
            exit 1
          fi

          # Run evaluation 5x for statistical power (t-distribution)
          echo "Running candidate evaluation 5 times..."
          SCORES=""
          LAST_RESULT=""
          for i in 1 2 3 4 5; do
            echo "Candidate evaluation run $i/5..."
            EVAL_STDERR="/tmp/eval-stderr-candidate-t2-$i.log"
            EVAL_EXIT=0
            RESULT=$(./tests/e2e/evaluate.sh "$SCENARIO" "$OUTPUT_FILE" --json 2>"$EVAL_STDERR") || EVAL_EXIT=$?

            if [ "$EVAL_EXIT" -ne 0 ]; then
              echo "::warning::Candidate eval run $i exited with code $EVAL_EXIT"
              echo "::warning::Stderr: $(cat "$EVAL_STDERR")"
            fi

            # Check for evaluation error
            if echo "$RESULT" | jq -e '.error == true' > /dev/null 2>&1; then
              echo "::error::Candidate eval run $i failed: $(echo "$RESULT" | jq -r '.summary')"
              exit 1
            fi

            if echo "$RESULT" | jq -e '.score' > /dev/null 2>&1; then
              SCORE=$(echo "$RESULT" | jq -r '.score')
            else
              echo "::error::Candidate eval run $i returned invalid JSON"
              echo "Result preview: ${RESULT:0:500}"
              exit 1
            fi

            # Score bounds check (max 11 for UI scenarios, 10 for standard)
            if [ "$(echo "$SCORE < 0 || $SCORE > 11" | bc -l)" -eq 1 ]; then
              echo "::error::Integrity Check Failed: Score $SCORE out of bounds [0-11]"
              exit 1
            fi

            SCORES="$SCORES $SCORE"
            LAST_RESULT="$RESULT"
            echo "  Run $i score: $SCORE"
            sleep 1
          done

          SCORES_DISPLAY=$(echo "$SCORES" | sed 's/^ //')

          # Calculate 95% confidence interval
          CI_RESULT=$(calculate_confidence_interval "$SCORES_DISPLAY")
          AVG=$(get_mean "$SCORES_DISPLAY")
          CI_LOWER=$(get_ci_lower "$SCORES_DISPLAY")
          CI_UPPER=$(get_ci_upper "$SCORES_DISPLAY")

          echo "Candidate: $CI_RESULT from [$SCORES_DISPLAY]"
          echo "candidate_score=$AVG" >> $GITHUB_OUTPUT
          echo "candidate_scores=$SCORES_DISPLAY" >> $GITHUB_OUTPUT
          echo "candidate_ci=$CI_RESULT" >> $GITHUB_OUTPUT
          echo "candidate_ci_lower=$CI_LOWER" >> $GITHUB_OUTPUT
          echo "candidate_ci_upper=$CI_UPPER" >> $GITHUB_OUTPUT

          # Save evaluation result for score history recording
          echo "$LAST_RESULT" > "${RUNNER_TEMP:-/tmp}/eval-result-candidate.json"

          # Extract criteria breakdown with color-coded emoji indicators
          CRITERIA=$(echo "$LAST_RESULT" | jq -r '
            .criteria | to_entries | map(
              (if .value.points == .value.max then ":green_circle:"
               elif .value.points > 0 then ":yellow_circle:"
               else ":red_circle:" end) as $emoji |
              "| \($emoji) | \(.key) | \(.value.points)/\(.value.max) | \(.value.evidence) |"
            ) | join("\n")' 2>/dev/null || echo "| :warning: | criteria | N/A | Could not parse |")
          {
            echo "criteria<<EOF"
            echo "$CRITERIA"
            echo "EOF"
          } >> $GITHUB_OUTPUT

          # Extract SDP scores from last result
          SDP_SCORE=$(echo "$LAST_RESULT" | jq -r '.sdp.adjusted // .score // 0')
          SDP_EXTERNAL=$(echo "$LAST_RESULT" | jq -r '.sdp.external_benchmark // 75')
          SDP_EXTERNAL_CHANGE=$(echo "$LAST_RESULT" | jq -r '.sdp.external_change // "0%"')
          SDP_ROBUSTNESS=$(echo "$LAST_RESULT" | jq -r '.sdp.robustness // 1.0')
          SDP_INTERPRETATION=$(echo "$LAST_RESULT" | jq -r '.sdp.interpretation // "STABLE"')

          echo "SDP Score: $SDP_SCORE (external: $SDP_EXTERNAL, robustness: $SDP_ROBUSTNESS)"
          echo "sdp_score=$SDP_SCORE" >> $GITHUB_OUTPUT
          echo "sdp_external=$SDP_EXTERNAL" >> $GITHUB_OUTPUT
          echo "sdp_external_change=$SDP_EXTERNAL_CHANGE" >> $GITHUB_OUTPUT
          echo "sdp_robustness=$SDP_ROBUSTNESS" >> $GITHUB_OUTPUT
          echo "sdp_interpretation=$SDP_INTERPRETATION" >> $GITHUB_OUTPUT

      - name: Record score to history (Tier 2)
        continue-on-error: true
        working-directory: pr-branch
        run: |
          SCORE="${{ steps.eval-candidate.outputs.candidate_score }}"
          SCENARIO="${{ steps.select-scenario.outputs.scenario_name }}"

          RESULT_FILE="${RUNNER_TEMP:-/tmp}/eval-result-candidate.json"
          if [ -f "$RESULT_FILE" ]; then
            CRIT=$(jq -c '.criteria // {}' "$RESULT_FILE" 2>/dev/null || echo "{}")
          else
            CRIT="{}"
          fi

          SDP_ADJ="${{ steps.eval-candidate.outputs.sdp_score }}"
          SDP_EXT="${{ steps.eval-candidate.outputs.sdp_external }}"
          SDP_ROB="${{ steps.eval-candidate.outputs.sdp_robustness }}"
          SDP_JSON=$(jq -n \
            --argjson adj "${SDP_ADJ:-0}" \
            --argjson ext "${SDP_EXT:-75}" \
            --argjson rob "${SDP_ROB:-1.0}" \
            '{adjusted: $adj, external_benchmark: $ext, robustness: $rob}')

          jq -nc \
            --arg ts "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --arg scenario "$SCENARIO" \
            --argjson score "${SCORE:-0}" \
            --argjson max_score 10 \
            --argjson criteria "$CRIT" \
            --argjson sdp "$SDP_JSON" \
            '{timestamp: $ts, scenario: $scenario, score: $score, max_score: $max_score, criteria: $criteria, sdp: $sdp}' \
            >> tests/e2e/score-history.jsonl

          echo "Score recorded: $SCORE for $SCENARIO (Tier 2)"

      - name: Commit score history (Tier 2)
        continue-on-error: true
        working-directory: pr-branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add tests/e2e/score-history.jsonl
          git diff --staged --quiet || git commit -m "chore: record e2e score for ${{ steps.select-scenario.outputs.scenario_name }}"
          git push || echo "Nothing to push"

      - name: Generate score trends report (Tier 2)
        continue-on-error: true
        working-directory: pr-branch
        run: |
          if [ -x "tests/e2e/score-analytics.sh" ]; then
            ./tests/e2e/score-analytics.sh --report > SCORE_TRENDS.md 2>/dev/null || true
            echo "Score trends report generated"
          fi

      - name: Compute historical context (Tier 2)
        id: historical
        continue-on-error: true
        working-directory: pr-branch
        run: |
          SCENARIO="${{ steps.select-scenario.outputs.scenario_name }}"
          HISTORY="tests/e2e/score-history.jsonl"

          if [ ! -s "$HISTORY" ]; then
            echo "context=No historical data yet" >> $GITHUB_OUTPUT
            exit 0
          fi

          SCEN_DATA=$(jq -r "select(.scenario == \"$SCENARIO\") | .score" "$HISTORY" 2>/dev/null)
          if [ -z "$SCEN_DATA" ]; then
            echo "context=First run for this scenario" >> $GITHUB_OUTPUT
            exit 0
          fi

          SCEN_AVG=$(echo "$SCEN_DATA" | awk '{s+=$1; n++} END {if(n>0) printf "%.1f", s/n; else print "N/A"}')
          SCEN_COUNT=$(echo "$SCEN_DATA" | wc -l | tr -d ' ')

          WEAKEST=$(jq -r '.criteria | to_entries[] | "\(.key) \(.value.points) \(.value.max)"' "$HISTORY" 2>/dev/null | \
            awk '{sum[$1]+=$2; max[$1]+=$3} END {worst=""; worst_pct=100; for(k in sum){if(max[k]>0){pct=sum[k]*100/max[k]; if(pct<worst_pct){worst=k; worst_pct=pct}}} printf "%s (%.0f%%)", worst, worst_pct}' 2>/dev/null || echo "N/A")

          {
            echo "context<<EOF"
            echo "This scenario avg: $SCEN_AVG ($SCEN_COUNT runs)"
            echo "Weakest criterion: $WEAKEST"
            echo "EOF"
          } >> $GITHUB_OUTPUT

      - name: Compare scores (full evaluation with CI)
        id: compare
        working-directory: pr-branch
        run: |
          # Source stats library for CI comparison
          source tests/e2e/lib/stats.sh

          HAS_BASELINE="${{ steps.check-baseline.outputs.has_baseline }}"
          CANDIDATE="${{ steps.eval-candidate.outputs.candidate_score }}"
          CANDIDATE_CI="${{ steps.eval-candidate.outputs.candidate_ci }}"
          CANDIDATE_CI_LOWER="${{ steps.eval-candidate.outputs.candidate_ci_lower }}"
          CANDIDATE_CI_UPPER="${{ steps.eval-candidate.outputs.candidate_ci_upper }}"
          CANDIDATE_SCORES="${{ steps.eval-candidate.outputs.candidate_scores }}"
          CANDIDATE=${CANDIDATE:-0}

          # Handle bootstrapping scenario (no baseline wizard in main)
          if [ "$HAS_BASELINE" != "true" ]; then
            echo "BOOTSTRAPPING: No baseline wizard to compare against"
            echo "is_bootstrapping=true" >> $GITHUB_OUTPUT
            echo "baseline=N/A" >> $GITHUB_OUTPUT
            echo "baseline_ci=N/A" >> $GITHUB_OUTPUT
            echo "baseline_ci_lower=N/A" >> $GITHUB_OUTPUT
            echo "baseline_ci_upper=N/A" >> $GITHUB_OUTPUT
            echo "candidate=$CANDIDATE" >> $GITHUB_OUTPUT
            echo "candidate_ci=$CANDIDATE_CI" >> $GITHUB_OUTPUT
            echo "candidate_ci_lower=$CANDIDATE_CI_LOWER" >> $GITHUB_OUTPUT
            echo "candidate_ci_upper=$CANDIDATE_CI_UPPER" >> $GITHUB_OUTPUT
            echo "delta=N/A" >> $GITHUB_OUTPUT
            echo "status=BOOTSTRAPPING" >> $GITHUB_OUTPUT
            echo "emoji=:seedling:" >> $GITHUB_OUTPUT
            echo "pass=true" >> $GITHUB_OUTPUT
            echo "verdict=First wizard installation - no baseline to compare" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "is_bootstrapping=false" >> $GITHUB_OUTPUT
          BASELINE="${{ steps.eval-baseline.outputs.baseline_score }}"
          BASELINE_CI="${{ steps.eval-baseline.outputs.baseline_ci }}"
          BASELINE_CI_LOWER="${{ steps.eval-baseline.outputs.baseline_ci_lower }}"
          BASELINE_CI_UPPER="${{ steps.eval-baseline.outputs.baseline_ci_upper }}"
          BASELINE_SCORES="${{ steps.eval-baseline.outputs.baseline_scores }}"
          BASELINE=${BASELINE:-0}

          DELTA=$(echo "$CANDIDATE - $BASELINE" | bc -l)
          DELTA_FORMATTED=$(printf "%.1f" "$DELTA")

          echo "Baseline: $BASELINE_CI"
          echo "Candidate: $CANDIDATE_CI"
          echo "Delta: $DELTA_FORMATTED"

          # Use statistical CI comparison for verdict
          STAT_VERDICT=$(compare_ci "$BASELINE_SCORES" "$CANDIDATE_SCORES")

          case "$STAT_VERDICT" in
            "IMPROVED")
              STATUS="IMPROVED"
              EMOJI=":rocket:"
              PASS="true"
              VERDICT="Candidate's lower CI ($CANDIDATE_CI_LOWER) > baseline's upper CI ($BASELINE_CI_UPPER) = statistically significant improvement"
              ;;
            "REGRESSION")
              STATUS="REGRESSION"
              EMOJI=":x:"
              PASS="false"
              VERDICT="Candidate's upper CI ($CANDIDATE_CI_UPPER) < baseline's lower CI ($BASELINE_CI_LOWER) = statistically significant regression"
              ;;
            *)
              STATUS="STABLE"
              EMOJI=":white_check_mark:"
              PASS="true"
              VERDICT="CIs overlap = no statistically significant difference (stable)"
              ;;
          esac

          echo "baseline=$BASELINE" >> $GITHUB_OUTPUT
          echo "candidate=$CANDIDATE" >> $GITHUB_OUTPUT
          echo "baseline_ci=$BASELINE_CI" >> $GITHUB_OUTPUT
          echo "candidate_ci=$CANDIDATE_CI" >> $GITHUB_OUTPUT
          echo "baseline_ci_lower=$BASELINE_CI_LOWER" >> $GITHUB_OUTPUT
          echo "baseline_ci_upper=$BASELINE_CI_UPPER" >> $GITHUB_OUTPUT
          echo "candidate_ci_lower=$CANDIDATE_CI_LOWER" >> $GITHUB_OUTPUT
          echo "candidate_ci_upper=$CANDIDATE_CI_UPPER" >> $GITHUB_OUTPUT
          echo "delta=$DELTA_FORMATTED" >> $GITHUB_OUTPUT
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "emoji=$EMOJI" >> $GITHUB_OUTPUT
          echo "pass=$PASS" >> $GITHUB_OUTPUT
          echo "verdict=$VERDICT" >> $GITHUB_OUTPUT

      - name: Build full evaluation comment message
        id: build-full-message
        env:
          # CRITERIA and HISTORICAL_CONTEXT passed via env block to avoid
          # backtick command substitution — LLM evidence text contains
          # `npm test`, `git diff` etc. which bash executes if inline-expanded
          CRITERIA: ${{ steps.eval-candidate.outputs.criteria }}
          HISTORICAL_CONTEXT: ${{ steps.historical.outputs.context }}
        run: |
          IS_BOOTSTRAPPING="${{ steps.compare.outputs.is_bootstrapping }}"
          BASELINE="${{ steps.compare.outputs.baseline }}"
          CANDIDATE="${{ steps.compare.outputs.candidate }}"
          BASELINE_CI="${{ steps.compare.outputs.baseline_ci }}"
          CANDIDATE_CI="${{ steps.compare.outputs.candidate_ci }}"
          BASELINE_CI_LOWER="${{ steps.compare.outputs.baseline_ci_lower }}"
          BASELINE_CI_UPPER="${{ steps.compare.outputs.baseline_ci_upper }}"
          CANDIDATE_CI_LOWER="${{ steps.compare.outputs.candidate_ci_lower }}"
          CANDIDATE_CI_UPPER="${{ steps.compare.outputs.candidate_ci_upper }}"
          BASELINE_SCORES="${{ steps.eval-baseline.outputs.baseline_scores }}"
          CANDIDATE_SCORES="${{ steps.eval-candidate.outputs.candidate_scores }}"
          DELTA="${{ steps.compare.outputs.delta }}"
          STATUS="${{ steps.compare.outputs.status }}"
          EMOJI="${{ steps.compare.outputs.emoji }}"
          VERDICT="${{ steps.compare.outputs.verdict }}"
          SDP_SCORE="${{ steps.eval-candidate.outputs.sdp_score }}"
          SDP_EXTERNAL="${{ steps.eval-candidate.outputs.sdp_external }}"
          SDP_EXTERNAL_CHANGE="${{ steps.eval-candidate.outputs.sdp_external_change }}"
          SDP_ROBUSTNESS="${{ steps.eval-candidate.outputs.sdp_robustness }}"
          SDP_INTERPRETATION="${{ steps.eval-candidate.outputs.sdp_interpretation }}"
          SCENARIO_NAME="${{ steps.select-scenario.outputs.scenario_name }}"

          if [ "$IS_BOOTSTRAPPING" = "true" ]; then
            {
              echo "message<<EOF"
              echo "## E2E Full Evaluation (Tier 2) $EMOJI"
              echo ""
              echo "_First wizard installation - no baseline to compare against._"
              echo ""
              echo "> **Scenario:** \`$SCENARIO_NAME\`"
              echo ""
              echo "| Layer | Metric | Value |"
              echo "|-------|--------|-------|"
              echo "| **L1: Model** | External Benchmark | $SDP_EXTERNAL ($SDP_EXTERNAL_CHANGE) |"
              echo "| **L2: SDLC** | Candidate (PR) | $CANDIDATE_CI |"
              echo "| | SDP (adjusted) | $SDP_SCORE / 10 |"
              echo "| | 95% CI | [$CANDIDATE_CI_LOWER, $CANDIDATE_CI_UPPER] |"
              echo "| | Individual Runs | $CANDIDATE_SCORES |"
              echo "| **Combined** | Robustness | $SDP_ROBUSTNESS |"
              echo "| | Status | **BOOTSTRAPPING** |"
              echo ""
              echo "### Result: First wizard installation verified (5x evaluation)"
              echo ""
              echo "### Criteria Breakdown (Candidate)"
              echo ""
              echo "| | Criterion | Score | Evidence |"
              echo "|---|-----------|-------|----------|"
              echo "$CRITERIA"
              echo ""
              echo "This PR introduces the wizard for the first time. After merge, future PRs will have a baseline to compare against."
              echo ""
              if [ -n "$HISTORICAL_CONTEXT" ] && [ "$HISTORICAL_CONTEXT" != "No historical data yet" ]; then
                echo "<details>"
                echo "<summary>Historical Context</summary>"
                echo ""
                echo "$HISTORICAL_CONTEXT"
                echo ""
                echo "</details>"
                echo ""
              fi
              echo "---"
              echo "*Bootstrapping mode: Candidate-only evaluation with 5x runs + 95% CI + SDP scoring.*"
              echo "EOF"
            } >> $GITHUB_OUTPUT
          else
            # Determine status text
            case "$STATUS" in
              "IMPROVED") STATUS_TEXT="Statistically significant IMPROVEMENT - ready to merge!" ;;
              "STABLE") STATUS_TEXT="No significant difference - safe to merge" ;;
              "REGRESSION") STATUS_TEXT="Statistically significant REGRESSION - do not merge" ;;
              *) STATUS_TEXT="$STATUS" ;;
            esac

            # Robustness interpretation (validate numeric before bc comparison)
            ROBUSTNESS_NOTE=""
            if [ -n "$SDP_ROBUSTNESS" ] && echo "$SDP_ROBUSTNESS" | grep -qE '^-?[0-9]+\.?[0-9]*$'; then
              if [ "$SDP_ROBUSTNESS" != "1.0" ]; then
                if [ "$(echo "$SDP_ROBUSTNESS < 0.8" | bc -l 2>/dev/null || echo 0)" = "1" ]; then
                  ROBUSTNESS_NOTE=" (ROBUST)"
                elif [ "$(echo "$SDP_ROBUSTNESS > 1.2" | bc -l 2>/dev/null || echo 0)" = "1" ]; then
                  ROBUSTNESS_NOTE=" (FRAGILE)"
                fi
              fi
            fi

            {
              echo "message<<EOF"
              echo "## E2E Full Evaluation (Tier 2) $EMOJI"
              echo ""
              echo "_Thorough pre-merge check with 5x evaluation runs + 95% confidence intervals + SDP model tracking._"
              echo ""
              echo "> **Scenario:** \`$SCENARIO_NAME\`"
              echo ""
              echo "### Model Context (Layer 1)"
              echo "| Metric | Value |"
              echo "|--------|-------|"
              echo "| External Benchmark | $SDP_EXTERNAL ($SDP_EXTERNAL_CHANGE vs baseline) |"
              echo "| Interpretation | $SDP_INTERPRETATION |"
              echo ""
              echo "### SDLC Scores (Layer 2)"
              echo "| Metric | Baseline (main) | Candidate (PR) |"
              echo "|--------|-----------------|----------------|"
              echo "| Raw Score | $BASELINE_CI | $CANDIDATE_CI |"
              echo "| SDP (adjusted) | - | $SDP_SCORE / 10 |"
              echo "| 95% CI | [$BASELINE_CI_LOWER, $BASELINE_CI_UPPER] | [$CANDIDATE_CI_LOWER, $CANDIDATE_CI_UPPER] |"
              echo "| Runs | $BASELINE_SCORES | $CANDIDATE_SCORES |"
              echo ""
              echo "### Combined Analysis"
              echo "| Metric | Value |"
              echo "|--------|-------|"
              echo "| Robustness | $SDP_ROBUSTNESS$ROBUSTNESS_NOTE |"
              echo "| Statistical Verdict | **$STATUS** $EMOJI |"
              echo ""
              echo "**Verdict:** $VERDICT"
              echo ""
              echo "### Status: $STATUS_TEXT"
              echo ""
              echo "### Criteria Breakdown (Candidate)"
              echo ""
              echo "| | Criterion | Score | Evidence |"
              echo "|---|-----------|-------|----------|"
              echo "$CRITERIA"
              echo ""
              if [ -n "$HISTORICAL_CONTEXT" ] && [ "$HISTORICAL_CONTEXT" != "No historical data yet" ]; then
                echo "<details>"
                echo "<summary>Historical Context</summary>"
                echo ""
                echo "$HISTORICAL_CONTEXT"
                echo ""
                echo "</details>"
                echo ""
              fi
              echo "---"
              echo "*Tier 2: 5x runs with t-distribution 95% CI. SDP adjusts for model conditions. Robustness < 1.0 = SDLC resilient.*"
              echo "EOF"
            } >> $GITHUB_OUTPUT
          fi

      - name: Comment full evaluation results on PR
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: e2e-full-eval
          message: ${{ steps.build-full-message.outputs.message }}

      - name: Fail on regression
        if: steps.compare.outputs.pass != 'true'
        run: |
          echo "Full E2E evaluation detected a regression"
          echo "Baseline: ${{ steps.compare.outputs.baseline }}"
          echo "Candidate: ${{ steps.compare.outputs.candidate }}"
          echo "Delta: ${{ steps.compare.outputs.delta }}"
          echo "DO NOT MERGE - regression detected"
          exit 1
