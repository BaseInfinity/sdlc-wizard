name: Monthly Research Deep Dive

on:
  workflow_dispatch:  # Auto-schedule disabled until roadmap items 15-22 complete

permissions:
  contents: write
  issues: write
  id-token: write

jobs:
  deep-research:
    runs-on: ubuntu-latest

    outputs:
      has_updates: ${{ steps.parse.outputs.updates_count != '0' }}
      updates_count: ${{ steps.parse.outputs.updates_count }}
      current_month: ${{ steps.current-month.outputs.current_month }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Get current month
        id: current-month
        run: |
          CURRENT_MONTH=$(date +%Y-%m)
          echo "current_month=$CURRENT_MONTH" >> $GITHUB_OUTPUT
          echo "Current month: $CURRENT_MONTH"

      - name: Run deep research with Claude
        id: research
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          prompt: |
            # Monthly Deep Research: AI-Assisted Development Trends

            **Month:** ${{ steps.current-month.outputs.current_month }}

            ## Research Goals

            Perform deep research on AI-assisted software development trends that could
            improve the SDLC Wizard. Focus on actionable insights, not general news.

            ## Research Areas

            ### 1. Academic Papers & Research
            Search for recent papers on:
            - AI code generation quality and reliability
            - Human-AI collaboration in software development
            - Test-driven development with AI assistance
            - Confidence calibration in AI systems

            ### 2. Major Announcements
            Check for significant releases from:
            - Anthropic (Claude updates, new features)
            - OpenAI (competitive landscape)
            - GitHub (Copilot features, Actions updates)
            - Major IDE vendors (VS Code, JetBrains)

            ### 3. Community Deep Dive
            Go deeper than weekly scans:
            - Reddit: r/ClaudeAI, r/programming, r/ExperiencedDevs
            - Hacker News: Top discussions about AI coding
            - Dev.to, Medium: Practitioner experiences
            - YouTube: Conference talks, tutorials

            ### 4. Emerging Patterns
            Look for patterns in how teams are using AI coding assistants:
            - Workflow improvements
            - Common pitfalls
            - Best practices emerging
            - Tool combinations that work well

            ## Output Format

            Return JSON:
            ```json
            {
              "month": "YYYY-MM",
              "research_summary": "Brief overview of findings",
              "papers": [
                {
                  "title": "Paper title",
                  "source": "arxiv/conference/journal",
                  "url": "link if available",
                  "relevance": "Why this matters for SDLC Wizard",
                  "actionable_insight": "What we could implement"
                }
              ],
              "announcements": [
                {
                  "source": "Company/Project",
                  "what": "Description",
                  "impact": "How this affects us",
                  "action_needed": true/false
                }
              ],
              "community_trends": [
                {
                  "trend": "Description",
                  "evidence": ["link1", "link2"],
                  "relevance": "HIGH/MEDIUM/LOW",
                  "suggested_response": "What we should do"
                }
              ],
              "recommended_wizard_updates": [
                {
                  "area": "hooks/skills/docs/philosophy",
                  "change": "What to change",
                  "rationale": "Why",
                  "priority": "HIGH/MEDIUM/LOW"
                }
              ],
              "nothing_notable": false
            }
            ```

            ## Guidelines

            - Quality over quantity - only include genuinely useful findings
            - Be skeptical of hype - focus on proven patterns
            - Align with SDLC Wizard philosophy: KISS, TDD, confidence levels
            - If nothing notable found, set nothing_notable: true
            - Prioritize actionable insights over interesting-but-not-useful
          direct_prompt: true
          model: claude-sonnet-4-20250514
          allowed_tools: "WebSearch,WebFetch"

      - name: Save research result to file
        env:
          RESEARCH_RESPONSE: ${{ steps.research.outputs.response }}
        run: |
          printf '%s' "$RESEARCH_RESPONSE" > /tmp/research_result.json

      - name: Parse research result
        id: parse
        run: |
          NOTHING_NOTABLE=$(jq -r '.nothing_notable // false' /tmp/research_result.json 2>/dev/null || echo "false")
          PAPERS_COUNT=$(jq -r '.papers | length // 0' /tmp/research_result.json 2>/dev/null || echo "0")
          ANNOUNCEMENTS_COUNT=$(jq -r '.announcements | length // 0' /tmp/research_result.json 2>/dev/null || echo "0")
          TRENDS_COUNT=$(jq -r '.community_trends | length // 0' /tmp/research_result.json 2>/dev/null || echo "0")
          UPDATES_COUNT=$(jq -r '.recommended_wizard_updates | length // 0' /tmp/research_result.json 2>/dev/null || echo "0")

          echo "nothing_notable=$NOTHING_NOTABLE" >> $GITHUB_OUTPUT
          echo "papers_count=$PAPERS_COUNT" >> $GITHUB_OUTPUT
          echo "announcements_count=$ANNOUNCEMENTS_COUNT" >> $GITHUB_OUTPUT
          echo "trends_count=$TRENDS_COUNT" >> $GITHUB_OUTPUT
          echo "updates_count=$UPDATES_COUNT" >> $GITHUB_OUTPUT

          echo "Nothing notable: $NOTHING_NOTABLE"
          echo "Papers: $PAPERS_COUNT, Announcements: $ANNOUNCEMENTS_COUNT"
          echo "Trends: $TRENDS_COUNT, Updates: $UPDATES_COUNT"

      - name: Build issue body
        if: steps.parse.outputs.nothing_notable != 'true'
        run: |
          cat > /tmp/issue_body.md << 'ISSUEEOF'
          ## Monthly Research Report

          **Month:** ${{ steps.current-month.outputs.current_month }}

          | Category | Count |
          |----------|-------|
          | Papers | ${{ steps.parse.outputs.papers_count }} |
          | Announcements | ${{ steps.parse.outputs.announcements_count }} |
          | Community Trends | ${{ steps.parse.outputs.trends_count }} |
          | Recommended Updates | ${{ steps.parse.outputs.updates_count }} |

          ### Full Research Report
          ```json
          ISSUEEOF

          cat /tmp/research_result.json >> /tmp/issue_body.md

          cat >> /tmp/issue_body.md << 'ISSUEEOF'
          ```

          ### Review Guidelines
          - Papers: Consider if findings align with SDLC philosophy
          - Announcements: Check if action is needed
          - Trends: Validate against our experience
          - Updates: Evaluate each against KISS principle

          ---
          *Generated by monthly-research workflow*
          ISSUEEOF

      - name: Close stale research issues
        if: steps.parse.outputs.nothing_notable != 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Find open issues with monthly-research label (will be superseded by new one)
          STALE_ISSUES=$(gh issue list --label "monthly-research" --state open --json number --jq '.[].number' 2>/dev/null || echo "")

          for issue in $STALE_ISSUES; do
            echo "Closing stale issue #$issue (superseded by new research)"
            gh issue close "$issue" --comment "Superseded by newer monthly research: ${{ steps.current-month.outputs.current_month }}" || true
          done

      - name: Create research issue
        if: steps.parse.outputs.nothing_notable != 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh issue create \
            --title "Monthly Research: ${{ steps.current-month.outputs.current_month }}" \
            --label "monthly-research" \
            --label "auto-generated" \
            --body-file /tmp/issue_body.md

      - name: Log if nothing notable
        if: steps.parse.outputs.nothing_notable == 'true'
        run: |
          echo "No notable research findings this month."
          echo "This can happen - not every month has relevant developments."

  # E2E Testing: Validate research-suggested improvements
  e2e-test:
    needs: deep-research
    if: needs.deep-research.outputs.has_updates == 'true'
    runs-on: ubuntu-latest

    outputs:
      verdict: ${{ steps.verdict.outputs.verdict }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Claude Code
        run: |
          npm install -g @anthropic-ai/claude-code
          claude --version

      - name: Setup test fixture with wizard
        run: |
          echo "Installing wizard into test fixture..."
          mkdir -p tests/e2e/fixtures/test-repo/.claude
          cp -r .claude/hooks tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp -r .claude/skills tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true
          cp .claude/settings.json tests/e2e/fixtures/test-repo/.claude/ 2>/dev/null || true

      # Baseline: Current SDLC docs
      - name: Run baseline simulation with Claude
        id: baseline-sim
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          working_directory: tests/e2e/fixtures/test-repo
          claude_args: |
            --max-turns 30
            --allowedTools "Read,Edit,Write,Bash(npm *),Bash(node *)"
          prompt: |
            Run an E2E SDLC simulation.

            Scenario file: ../scenarios/medium-add-feature.md

            1. Read the scenario file to understand the task
            2. Execute the scenario task following SDLC principles
            3. Return results as JSON with score

            After execution, output a summary of what you did and whether SDLC was followed.
          model: claude-sonnet-4-20250514

      - name: Baseline - Current SDLC (Tier 2)
        id: baseline
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          echo "Running baseline evaluation with current SDLC docs"

          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"

          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "No output file from baseline simulation"
            echo "scores=0" >> $GITHUB_OUTPUT
            echo "score=0" >> $GITHUB_OUTPUT
            echo "ci=0 (no data)" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Use shared evaluation script with output file
          RESULT=$(./tests/e2e/run-tier2-evaluation.sh \
            tests/e2e/scenarios/medium-add-feature.md \
            "$OUTPUT_FILE")

          SCORES=$(echo "$RESULT" | grep '^scores=' | cut -d= -f2)
          MEAN=$(echo "$RESULT" | grep '^score=' | cut -d= -f2)
          CI_RESULT=$(echo "$RESULT" | grep '^ci=' | cut -d= -f2-)

          echo "scores=$SCORES" >> $GITHUB_OUTPUT
          echo "score=$MEAN" >> $GITHUB_OUTPUT
          echo "ci=$CI_RESULT" >> $GITHUB_OUTPUT
          echo "Baseline: $CI_RESULT"

      # Apply research recommendations
      - name: Apply research recommendations
        id: apply
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          prompt: |
            Based on this month's research findings, apply minimal targeted
            improvements to SDLC.md.

            Research updates count: ${{ needs.deep-research.outputs.updates_count }}

            Guidelines:
            - Only apply changes backed by research evidence
            - Keep changes minimal and focused
            - Prioritize HIGH priority recommendations
            - Don't rewrite entire sections
          direct_prompt: true
          model: claude-sonnet-4-20250514

      - name: Reset test fixture for candidate
        run: |
          cd tests/e2e/fixtures/test-repo
          git checkout -- . 2>/dev/null || true
          git clean -fd 2>/dev/null || true

      # Candidate: SDLC with research improvements
      - name: Run candidate simulation with Claude
        id: candidate-sim
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          working_directory: tests/e2e/fixtures/test-repo
          claude_args: |
            --max-turns 30
            --allowedTools "Read,Edit,Write,Bash(npm *),Bash(node *)"
          prompt: |
            Run an E2E SDLC simulation with research improvements applied.

            Scenario file: ../scenarios/medium-add-feature.md

            1. Read the scenario file to understand the task
            2. Execute the scenario task following SDLC principles
            3. Return results as JSON with score

            After execution, output a summary of what you did and whether SDLC was followed.
          model: claude-sonnet-4-20250514

      - name: Candidate - With Improvements (Tier 2)
        id: candidate
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          echo "Running candidate evaluation with research improvements"

          OUTPUT_FILE="${RUNNER_TEMP:-/tmp}/claude-execution-output.json"

          if [ ! -f "$OUTPUT_FILE" ]; then
            echo "No output file from candidate simulation"
            echo "scores=0" >> $GITHUB_OUTPUT
            echo "score=0" >> $GITHUB_OUTPUT
            echo "ci=0 (no data)" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Use shared evaluation script with output file
          RESULT=$(./tests/e2e/run-tier2-evaluation.sh \
            tests/e2e/scenarios/medium-add-feature.md \
            "$OUTPUT_FILE")

          SCORES=$(echo "$RESULT" | grep '^scores=' | cut -d= -f2)
          MEAN=$(echo "$RESULT" | grep '^score=' | cut -d= -f2)
          CI_RESULT=$(echo "$RESULT" | grep '^ci=' | cut -d= -f2-)

          echo "scores=$SCORES" >> $GITHUB_OUTPUT
          echo "score=$MEAN" >> $GITHUB_OUTPUT
          echo "ci=$CI_RESULT" >> $GITHUB_OUTPUT
          echo "Candidate: $CI_RESULT"

      - name: Determine verdict
        id: verdict
        run: |
          source tests/e2e/lib/stats.sh

          BASELINE_SCORES="${{ steps.baseline.outputs.scores }}"
          CANDIDATE_SCORES="${{ steps.candidate.outputs.scores }}"

          VERDICT=$(compare_ci "$BASELINE_SCORES" "$CANDIDATE_SCORES")
          echo "verdict=$VERDICT" >> $GITHUB_OUTPUT
          echo "Verdict: $VERDICT"

      - name: Update CUSUM
        run: |
          SCORE="${{ steps.baseline.outputs.score }}"
          if [ -n "$SCORE" ] && [ "$SCORE" != "0" ]; then
            ./tests/e2e/cusum.sh --add "$SCORE" || true
          fi

      - name: Create PR with results
        if: steps.verdict.outputs.verdict == 'IMPROVED' || steps.verdict.outputs.verdict == 'STABLE'
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "feat(sdlc): incorporate research findings"
          title: "[${{ steps.verdict.outputs.verdict }}] Research Update: ${{ needs.deep-research.outputs.current_month }}"
          body: |
            ## Monthly Research Integration

            **Month:** ${{ needs.deep-research.outputs.current_month }}

            ### E2E Test Results

            | Phase | Score (95% CI) |
            |-------|----------------|
            | Baseline | ${{ steps.baseline.outputs.ci }} |
            | With Changes | ${{ steps.candidate.outputs.ci }} |

            ### Verdict: **${{ steps.verdict.outputs.verdict }}**

            - **IMPROVED**: Research findings significantly improve SDLC scores
            - **STABLE**: No significant difference (merge if findings are valuable)
            - **REGRESSION**: Changes hurt scores (don't merge)

            ### Research Applied
            Based on ${{ needs.deep-research.outputs.updates_count }} recommended updates from this month's research.

            ---
            *Generated by monthly-research workflow with E2E validation*
          branch: research-update/${{ needs.deep-research.outputs.current_month }}
          labels: |
            monthly-research
            e2e-tested
            ${{ steps.verdict.outputs.verdict }}
